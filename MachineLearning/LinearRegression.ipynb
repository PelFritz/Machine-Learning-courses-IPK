{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LinearRegression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPI8HZI+kcCjsfYFDylVYAu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ahTZrelxs4ir"},"source":["**Linear Regression**\n","\n","---\n","One very familiar analysis to biologist/bioinformaticians is the regression analysis. Whether in quantitative genetics, metabolomics or bioinformatics in general, there is never a shortage of problems requiring regression analysis.\n","\n","A regression analysis establishes a relationship between one or several independent variables and a ***continuous*** dependent variable. The word continuous draws the thin line between regression and classification, the latter which establishes a relationship to ***categorical*** dependent variables.\n","\n","\n","---\n","1. In this notebook we will be performing a linear regression analysis on simulated gene expression data. In this dataset, 100 genes have been simulated to explain the variation within an arbitrary phenotype. \n","\n","2. Some gaussian noise was added to the data to mimic what we have in real life, since real life data always has some kind of noise, whose origin is unknown to us.\n","\n","3. Only a few of these genes are actually important in these dataset. There are a group of regression methods that can take care of such not important variables called regularized regression models. We will talk about them next time.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HVb3z8ArxAHP"},"source":["**We Import our Libraries/Packages/Modules**\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"g5IXR4xkszaD","executionInfo":{"status":"ok","timestamp":1627575350807,"user_tz":-120,"elapsed":467,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}}},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import explained_variance_score\n","pd.options.display.width=0"],"execution_count":77,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSdEl3Urx4W3","executionInfo":{"status":"ok","timestamp":1627575351728,"user_tz":-120,"elapsed":462,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"3072cef5-e1ce-4f64-ca97-9f0d24e1e24e"},"source":["# We import our data from our Github repository\n","expression_data = pd.read_csv('https://raw.githubusercontent.com/PelFritz/Machine-Learning-courses-IPK/master/Data/simulated_gene_expression.csv')\n","print(expression_data.head())"],"execution_count":78,"outputs":[{"output_type":"stream","text":["      Gene 1     Gene 2     Gene 3  ...    Gene 99   Gene 100    Phenotype\n","0  20.679180  20.386841  19.177778  ...  19.781276  19.326157   730.946537\n","1  22.044115  20.231690  19.672736  ...  20.186680  19.283732  1162.961935\n","2  20.220710  19.030321  20.053268  ...  20.534897  18.834098   777.562283\n","3  18.553638  19.176032  19.316308  ...  18.103340  18.941041   426.680033\n","4  20.406725  20.854890  19.416488  ...  19.125939  19.788676   686.734064\n","\n","[5 rows x 101 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5fI_9hvsyqRk"},"source":["Our data contains the 101, columns. The last column is the arbitrary phenotype we spoke of. We will separate our independent variables (genes) from our dependent variable(phenotype)."]},{"cell_type":"code","metadata":{"id":"lDaWoAdhzDIF","executionInfo":{"status":"ok","timestamp":1627575351729,"user_tz":-120,"elapsed":12,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}}},"source":["x = expression_data.values[:, :-1]\n","y = expression_data.values[:, -1]\n"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sceJ2hiGzWt3"},"source":["**In machine learning, we always need to scale our data. What is scaling?**\n","\n","---\n","Whenever we get data, every column/independent variable can have a different scale. By scale we mean some variables may have values from 1 to 20, others from 1 to 1000 and others in the hundreds of thousands. Feeding your machine learning algorithm such data may make the algorithm feel that variables in the hundreds of thousand scale are\n","more important than those in the tens. There are many ways to approach such problem in machine learning:\n","\n","1. **standardization** : Standardization centers every variable at zero and scales it to unit variance. \"*I know big words for a simple calculation*\". To standardize a variable, we subtract the mean from every value and divide by the standard deviation **(x - mean / standard deviation)**. \n","\n","2. **Normalisation** : I prefer to call it sklearn's name which is the MinMax scaling. Here we scale every variable to fall between a chosen min and max value. For neural networks min=0 and max=1 is preferred. To scale to 0 - 1, we can simply divide every column by the max value in the column. In image analysis Deep learning experts just usually divide every pixel by 255, since RGB images usually have pixels from 0 to 255.\n","\n","3. There are other scaling methods such as the **robust scaling**. For more scalers see [Sklearn scalers](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)\n"]},{"cell_type":"code","metadata":{"id":"9YWfJIJ33C2W","executionInfo":{"status":"ok","timestamp":1627575351730,"user_tz":-120,"elapsed":11,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}}},"source":["scaler = StandardScaler()\n","x_std = scaler.fit_transform(x)"],"execution_count":80,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2SXDu8P8-PUY"},"source":["Let us look at summary statistics for the standardized vs non standardized data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U9XnZVJd-YgV","executionInfo":{"status":"ok","timestamp":1627575352562,"user_tz":-120,"elapsed":842,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"5b872597-538e-4fec-db6e-0c72fd39f58e"},"source":["not_std_x = pd.DataFrame(x)\n","std_x = pd.DataFrame(x_std)\n","print('Not standardized data statistics')\n","print(not_std_x.describe())\n","print('\\n Standardized data statistics')\n","print(std_x.describe())"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Not standardized data statistics\n","               0           1           2   ...          97          98          99\n","count  539.000000  539.000000  539.000000  ...  539.000000  539.000000  539.000000\n","mean    19.996930   19.976911   20.009410  ...   19.933319   19.997363   19.997700\n","std      1.044393    1.011574    1.034263  ...    1.065841    1.067269    1.059661\n","min     16.618052   17.252678   16.590548  ...   17.222950   16.470486   16.884465\n","25%     19.327183   19.283846   19.340970  ...   19.223886   19.285878   19.204479\n","50%     19.993799   19.997293   19.932785  ...   19.856265   19.990561   19.997625\n","75%     20.675377   20.698781   20.715921  ...   20.636688   20.744568   20.749940\n","max     23.211884   22.928608   23.149988  ...   24.405529   23.216354   22.659027\n","\n","[8 rows x 100 columns]\n","\n"," Standardized data statistics\n","                 0             1   ...            98            99\n","count  5.390000e+02  5.390000e+02  ...  5.390000e+02  5.390000e+02\n","mean   2.356392e-16  4.318129e-15  ... -7.569702e-16  7.353425e-17\n","std    1.000929e+00  1.000929e+00  ...  1.000929e+00  1.000929e+00\n","min   -3.238259e+00 -2.695566e+00  ... -3.307650e+00 -2.940682e+00\n","25%   -6.418741e-01 -6.857719e-01  ... -6.672605e-01 -7.492564e-01\n","50%   -3.000344e-03  2.016768e-02  ... -6.379744e-03 -7.057271e-05\n","75%    6.502126e-01  7.142746e-01  ...  7.007591e-01  7.105468e-01\n","max    3.081158e+00  2.920637e+00  ...  3.018901e+00  2.513821e+00\n","\n","[8 rows x 100 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1E9PqGcz3Ryu"},"source":["**Now we build our linear regression model.**\n","\n","---\n","\n","1. We split our data into train and test set. This is very important in machine/deep learning. We fit our linear model to the training data, then evaluate how good it is on the test data. In our case, we will reserve 20% of the data for test.\n","\n","2. If you run the code below, you will realize that you get different result every run. This is because, the split is randomised and everytime you run the model, the train and test dataset will be different. For this reason it is always advisable to use **cross validation** to evaluate models. We will talk about cross validation some other day."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"41e51trU3rjz","executionInfo":{"status":"ok","timestamp":1627575352563,"user_tz":-120,"elapsed":21,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"80e516a1-d23f-435c-d1a9-47d4f64d06ba"},"source":["x_train, x_test, y_train, y_test = train_test_split(x_std, y, test_size=0.2)\n","linear_model = LinearRegression()\n","linear_model.fit(x_train, y_train)\n","r2_score = linear_model.score(x_test, y_test)\n","print(r2_score)"],"execution_count":82,"outputs":[{"output_type":"stream","text":["0.9039148793514352\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JJKrN8CA6Ba-"},"source":["**Lets look at the variance explained by our model.**\n","\n","---\n","\n","The R square score tells you the variance explained by a regression model. By default, the scoring metrics of sklearn's regression classes is the R2 score. In case you decided to use some other metrics like the mean squared error, you can still always get the explained variance using the code below. Have a look at [metrices](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for more on performance metrices on sklearn."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nltM3rxW6J8K","executionInfo":{"status":"ok","timestamp":1627575352564,"user_tz":-120,"elapsed":12,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"87d20aff-4a13-45cd-8f6d-1f57af89cfe2"},"source":["y_pred = linear_model.predict(x_test)\n","explained_variance = explained_variance_score(y_test, y_pred)\n","print(explained_variance)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["0.9047963949283083\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ikrjFy79CakB"},"source":["**Your Turn !!!**\n","\n","---\n","\n","Find an interesting data of your choice and play around with it. Try out some different performance metrices like the mean squared error. Connect with us if you get into problems and we will **DEBUG** together. \n","\n","***Cheers***"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ML_1.2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NTimbgv3C4Qu"},"source":["## **Logistic regression**\n","Logistic regression is another form of regression which differs from the classical linear regression we saw in the first lesson. While linear regression outputs/predicts continous values, logistic regression outputs probabilities. It is mainly used in binary classification tasks although we call it regression. In **binary classification** we have mainly two classes: **class 0**  and **class 1**. A probability can take any value from 0 to 1. \n","\n","In logistic regression we can perform binary classification by assigning predictions with probabilities greater than **0.5** to belong to class 1, else belonging to class 0. In linear regression we fit a straight line to our data points but in logistic regression we fit a logistic function to our data points. A logistic function has an S-shape. Watch this cool video from [statquest](https://www.youtube.com/watch?v=yIYKR4sgzI8)."]},{"cell_type":"code","metadata":{"id":"G6FW574tB7yL","executionInfo":{"status":"ok","timestamp":1614000699257,"user_tz":-60,"elapsed":1616,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}}},"source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o1W8FE7NXiJj"},"source":["We import our breast cancer dataset, we standardize it then we build a logistic regression model"]},{"cell_type":"code","metadata":{"id":"eGBUFljhXsD9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614000699260,"user_tz":-60,"elapsed":1608,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"8c01d5ac-a082-4f84-9ecc-479a2f8ea2e7"},"source":["bc_data = load_breast_cancer()\n","x = bc_data.data # data\n","y = bc_data.target # labels\n","print(x.shape)\n","print(y.shape)\n","print(bc_data.feature_names) # use this to get names of variables\n","print(bc_data.target_names) # use this to find the various classes"],"execution_count":2,"outputs":[{"output_type":"stream","text":["(569, 30)\n","(569,)\n","['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n"," 'mean smoothness' 'mean compactness' 'mean concavity'\n"," 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n"," 'radius error' 'texture error' 'perimeter error' 'area error'\n"," 'smoothness error' 'compactness error' 'concavity error'\n"," 'concave points error' 'symmetry error' 'fractal dimension error'\n"," 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n"," 'worst smoothness' 'worst compactness' 'worst concavity'\n"," 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n","['malignant' 'benign']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TwtNs2b8X-Jg"},"source":["We standardize data using Standard scaler"]},{"cell_type":"code","metadata":{"id":"P_aQ0nqNYC7i","executionInfo":{"status":"ok","timestamp":1614000699261,"user_tz":-60,"elapsed":1606,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}}},"source":["scaler = StandardScaler()\n","x_scaled = scaler.fit_transform(x)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nPfTgHLZY4ta"},"source":["We split our data into training and a test set, then build our logistic regression model. We do a special kind of splitting called stratify splitting. This splitting ensures that both training and test data have equal percentages of data from both classes."]},{"cell_type":"code","metadata":{"id":"7Um0BAF8ZAPM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614000699261,"user_tz":-60,"elapsed":1599,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"063d8e99-7bd6-4280-d924-06cefe8cece8"},"source":["x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.2, stratify=y)\n","logreg_clf = LogisticRegression()\n","logreg_clf.fit(x_train, y_train)\n","\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"8L8fKPtbadmi"},"source":["Let us predict the class of one observation from our test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lbvdE9Uman8i","executionInfo":{"status":"ok","timestamp":1614000699262,"user_tz":-60,"elapsed":1591,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"257a9e5b-bbdc-4247-9737-45e3832776b4"},"source":["predicted_class = logreg_clf.predict(x_test[5].reshape(1, -1))\n","predicted_prob = logreg_clf.predict_proba(x_test[5].reshape(1, -1))\n","print('Predicted class = ',predicted_class, ' and Predicted probabilities = ',predicted_prob, 'Real class = ', y_test[5])"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Predicted class =  [1]  and Predicted probabilities =  [[0.00271056 0.99728944]] Real class =  1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1h1cH9ITb-AS"},"source":["From the above we see that our logistic regression model predicts a higher probability for class 1 (**probability = 0.92**). The **predict** function above gives us the class prediction while the **predict_proba** function gives us the probabilities of belonging to class 0 or class 1."]},{"cell_type":"markdown","metadata":{"id":"_IP4sA3zdLWF"},"source":["Let us now predict the classes for all the observations in our test set. We will also see how well we perform in our predictions by measuring our accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8httUu87db-M","executionInfo":{"status":"ok","timestamp":1614000721590,"user_tz":-60,"elapsed":518,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"9d6c881f-07d1-4c96-cad4-65b229c54790"},"source":["predictions_test = logreg_clf.predict(x_test)\n","predictions_train = logreg_clf.predict(x_train)\n","accuracy_test = accuracy_score(y_test, predictions_test)*100\n","accuracy_train = accuracy_score(y_train, predictions_train)*100\n","print(accuracy_test, accuracy_train)\n","print(predictions_test)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["98.24561403508771 98.68131868131869\n","[1 1 1 1 1 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 1\n"," 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 0\n"," 1 0 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 1\n"," 1 1 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fdY_ZfTOGxxL","executionInfo":{"status":"ok","timestamp":1614000699593,"user_tz":-60,"elapsed":1907,"user":{"displayName":"Fritz Forbang","photoUrl":"","userId":"12957595507620375437"}},"outputId":"98b0a9f6-e240-4bbe-cc15-68b3e8d20489"},"source":["y_true = [1, 1, 1, 1, 1, 0]\n","y_pred = [1, 1, 1, 1, 1, 1]\n","print(accuracy_score(y_true, y_pred))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["0.8333333333333334\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zrMBq6_gHkIf"},"source":["**How to calculate accuracy**\n","\n","true positive + true negatives / n\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rmt7ZqzbeE1D"},"source":["On our test data alone we get almost 95% accuracy. That is impressive right."]},{"cell_type":"markdown","metadata":{"id":"167JAAU_jAnG"},"source":["**What you should have learned.**\n","\n","1. What is logistic regression\n","2. What it is used for.\n","3. How to build a logistic regression classifier with scikit learn."]},{"cell_type":"markdown","metadata":{"id":"w3I-ZfsDiI1r"},"source":["**Your turn !!!**\n","\n","Choose a dataset of your choice, build a logistic regression model using this dataset and show how much accuracy you can achieve. You can also select just some of the variables or use all the variables if you wish.\n","\n","**Hint**\n","\n","Sklearn provides some datasets if you wish to use one from there. Have a look at [sklearn_datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)."]}]}